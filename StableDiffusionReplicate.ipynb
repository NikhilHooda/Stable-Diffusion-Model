{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to use a Stable Diffusion Model"
      ],
      "metadata": {
        "id": "wuCXPwyT_9TB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X32vMtxrIKk_"
      },
      "source": [
        "Install the necessary libraries: diffusers==0.11.1, scipy, ftfy, accelerate and transformers so we can use them to run our code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdFAKsiwitkN"
      },
      "outputs": [],
      "source": [
        "#install libraries\n",
        "!pip install diffusers==0.2.4\n",
        "!pip install transformers scipy ftfy\n",
        "!pip install \"ipywidgets>=7,<8\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXGDl7zDHxyD"
      },
      "source": [
        "Next, all the imports needed from the libraries need to be called. We also need to log into Hugging Face as well (Hugging Face is a developer tool where developers can share AI and ML models and datasets):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o7RHAjSuk3b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331,
          "referenced_widgets": [
            "a118bc9bcedc4719995b56f518e90047",
            "e8a27601ceab44c783ead7744a5c1e72",
            "2a35c3978eb840efbfce3e29f118ab22",
            "6078ff4fce5344d1ba732f94ec490e98",
            "30aaf950001848fdbeb41f79a04193ec",
            "3660674291f9416d8c3245660c1b788d",
            "f0826dbf791b41eb9364fe931d07069c",
            "f243a7758e9745e7a57ea625174df41b",
            "35cfef600fd047efabcb7792288a4c19",
            "329a545eda274c6da9d7c18165db137c",
            "8bc426b9820a451987a5652bbcb1ff01",
            "545dfb2202f64d3598a88f7fe1251fca",
            "98f274feaea441f4a02a2854e8abfc84",
            "e469c7d4dd4d4bf5b3bf918a800b0d5d",
            "61987676a1fd4a29abd3b0acb4d0a50d",
            "a732383f0f3c44688d8651bf895af260",
            "42e237105be3436293bf293913aa9836"
          ]
        },
        "outputId": "8d0c8f68-258e-4ce7-834c-c1d7335b94ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid.\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image, ImageDraw\n",
        "import cv2\n",
        "import numpy as np\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "import torch\n",
        "from torch import autocast\n",
        "from torch.nn import functional as F\n",
        "from diffusers import StableDiffusionPipeline, AutoencoderKL\n",
        "from diffusers import UNet2DConditionModel, PNDMScheduler, LMSDiscreteScheduler\n",
        "from diffusers.schedulers.scheduling_ddim import DDIMScheduler\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from tqdm.auto import tqdm\n",
        "from huggingface_hub import notebook_login\n",
        "from google.colab import output\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "output.enable_custom_widget_manager()\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN-RM2c5N2rP"
      },
      "source": [
        "`StableDiffusionPipeline` is an end-to-end inference pipeline that you can use to generate images from text. An inference pipeline is a trained model which you can use to put through new data inputs, in this case these inputs are the text, and the pipeline will turn these inputs into desired outputs (images)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2H1cjtAnKr0"
      },
      "outputs": [],
      "source": [
        "#this code will be used to load up the ML/stable diffusion model\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16)  \n",
        "\n",
        "#sending model to GPU\n",
        "pipe = pipe.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssJ2v8SFQWU5"
      },
      "source": [
        "\n",
        "When we call 'pipe' in the code and give it the parameters 'prompt', essentially what is happening is our text is being inputted into the 'StableDiffusionPipeline' model. At this stage, the model is going through 50 steps/layers of diffusion. Since the model we are using is already trained, the model is basically starting from just noise (pixels) and with each step of diffusion it is removing more and more noise until we are left with our output pixels. These pixels are put through a \"decoder\" which then prints out the final image.\n",
        "\n",
        "To understand how a stable diffusion model fully works start to finish, please watch this video from 2:31 - 4:29 https://www.youtube.com/watch?v=ltLNYA3lWAQ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JD22FMEXusV_"
      },
      "outputs": [],
      "source": [
        "#prompt for our stable diffusion model (what the image will be based off of)\n",
        "\n",
        "\n",
        "prompt = \"A futuristic city with flying cars\"\n",
        "\n",
        "#pipe is the model that we just loaded and it is going to run the prompt through the model and generate an image (image[0]) and store it in the list\n",
        "image = pipe(prompt).images[0]\n",
        "\n",
        "#display/print the image below\n",
        "image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eJvJXnGxRBh"
      },
      "source": [
        "Running the above cell multiple times will give you a different image every time. If you want to choose your output you can pass a random seed to the pipeline. Every time you use the same seed you'll have the same image result. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzaDubGxZv1X"
      },
      "outputs": [],
      "source": [
        "#passing a random seed manually to the model so that everytime the model is run, the same image is displayed\n",
        "generator = torch.Generator(\"cuda\").manual_seed(1024)\n",
        "\n",
        "#same model as before is being run which generate an image using the prompt given, the only difference is that the 'generator' parameter is now passing the seed we inputted into the model, resulting in the same image being generated by the model everytime \n",
        "image = pipe(prompt, generator=generator).images[0]\n",
        "\n",
        "#display/print the image below\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4JU7NVMjhZ3"
      },
      "source": [
        "The next parameter that can be manipulated is the number of diffusion steps indicated by the `num_inference_steps` argument. In general, images are better and more detailedthe more steps you use, however less steps means much faster diffusion process. The model has the same seed number as the one above but with less diffusion steps so you see the change for yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGhwvaQUjh-a"
      },
      "outputs": [],
      "source": [
        "#same seed so we can come the difference in images with less diffusion steps from the image above\n",
        "generator = torch.Generator(\"cuda\").manual_seed(1024)\n",
        "\n",
        "#same model as before but with a new parameter resulting in 15 steps of diffusion rather than the default 50\n",
        "image = pipe(prompt, num_inference_steps=15, generator=generator).images[0]\n",
        "\n",
        "#display/print the image below\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_c-DMSI8A77"
      },
      "source": [
        "It is also possible to change the parameters so the images produced are not squares. By default, stable diffusion produces images that are 512 x 512 but this can easily be changed. To do so, simply change the height and width so you can create images in portrait or landscape for example. While changing these values, remember that height and width are multiples of 8 so your new values must follow this or it will not work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aS29UtJi81Yq"
      },
      "outputs": [],
      "source": [
        "#same model but with changed width parameters so that the image displayed by the model is in landscape\n",
        "image = pipe(prompt, height=512, width=768).images[0]\n",
        "\n",
        "#display/print the image below\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qRhuIes9XbT"
      },
      "source": [
        "Finally, it is also possible to generate multiple images for the same prompt, all that must be done is a list must be used with the same prompt repeated several times. This list is then sent to the pipeline instead of the string we used before. To do so, we will need to create our own function 'image_grid' to display the grid of images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OB_lNdkghHDY"
      },
      "outputs": [],
      "source": [
        "#import from the python imaging library\n",
        "from PIL import Image\n",
        "\n",
        "#define the image_grid function and store passed arguments\n",
        "def image_grid(imgs, rows, cols):\n",
        "\n",
        "#testing whether number of images in our list match up with the number of images we plan to display based on the number of rows and coluums we have (debugging)\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "#storing the length and width of one of the images in the list into variables 'w' and 'h'\n",
        "    w, h = imgs[0].size\n",
        "\n",
        "#create a one new big image with the dimensions large enough to fit all images in the list\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "#for loop used to paste the images into the big image accordingly   \n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "\n",
        "#return function to main code\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU4R2byvhOcO"
      },
      "source": [
        "Now, we can generate a grid image once having run the pipeline with a list of 3 prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lsY5K8rhOAa"
      },
      "outputs": [],
      "source": [
        "#number of images we want to generate\n",
        "num_images = 3\n",
        "\n",
        "#this is the prompt our images will be based on and it is multiplied by the number of images we want to generate\n",
        "prompt = [\"A city in the future with flying cars\"] * num_images\n",
        "\n",
        "#pipe is going to run the prompt through the model we loaded previously and generate an image (image[0]) and store it in the images list\n",
        "images = pipe(prompt).images\n",
        "\n",
        "#pass arguments into image_grid function\n",
        "grid = image_grid(images, rows=1, cols=3)\n",
        "\n",
        "#Call image_grid function \n",
        "grid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, here is how to generate a grid of images"
      ],
      "metadata": {
        "id": "4ZZWNPu2BZEq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RNGrTUUahWXR"
      },
      "outputs": [],
      "source": [
        "#declare how many columns and rows of images you would like\n",
        "num_cols = 3\n",
        "num_rows = 4\n",
        "\n",
        "#prompt that the images will be based off multiplied by the number of columns of images we want to generate\n",
        "prompt = [\"A city in the future with flying cars\"] * num_cols\n",
        "\n",
        "#create an empty list where the images will be stored\n",
        "all_images = []\n",
        "\n",
        "#Use for loop to iterate through number of rows of images you would like, each time it iterates through it will generate a 3 new images as the prompt was multiplied by the number of columns and then those images will be added to the list\n",
        "for i in range(num_rows):\n",
        "  images = pipe(prompt).images\n",
        "  all_images.extend(images)\n",
        "\n",
        "#pass arguments into image_grid function\n",
        "grid = image_grid(a ll_images, rows=num_rows, cols=num_cols)\n",
        "\n",
        "#Call image_grid function \n",
        "grid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Pipeline from Scratch"
      ],
      "metadata": {
        "id": "ShbV-ah-Bimj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a custom pipeline, each individual part of the model will need to be downloaded, this includes the autoencoder, tokenizer, Unet and the scheduler. "
      ],
      "metadata": {
        "id": "pBuWCEkt1tZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the autoencoder model which will be used to decode the latents into image space. \n",
        "vae = AutoencoderKL.from_pretrained(\n",
        "    'CompVis/stable-diffusion-v1-4', subfolder='vae', use_auth_token=True)\n",
        "vae = vae.to(device)\n",
        "\n",
        "# 2. Load the tokenizer and text encoder to tokenize and encode the text. \n",
        "tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14')\n",
        "text_encoder = CLIPTextModel.from_pretrained('openai/clip-vit-large-patch14')\n",
        "text_encoder = text_encoder.to(device)\n",
        "\n",
        "# 3. The UNet model for generating the latents.\n",
        "unet = UNet2DConditionModel.from_pretrained(\n",
        "    'CompVis/stable-diffusion-v1-4', subfolder='unet', use_auth_token=True)\n",
        "unet = unet.to(device)\n",
        "\n",
        "# 4. Create a scheduler for inference\n",
        "scheduler = LMSDiscreteScheduler(\n",
        "    beta_start=0.00085, beta_end=0.012,\n",
        "    beta_schedule='scaled_linear', num_train_timesteps=1000)"
      ],
      "metadata": {
        "id": "f9Hs_NGlxlcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that all the necessary components of the model have been downloaded, the text prompt needs to be turned into embeddings which the computer can recognize. This will be done by putting the text prompt through the tokenizer which will turn the text into tokens, and then these tokens can be put through the autoencoder in order to get our embeddings. "
      ],
      "metadata": {
        "id": "xi17OPyr2QP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_embeds(prompt):\n",
        "  # Tokenize text and get embeddings\n",
        "  text_input = tokenizer(\n",
        "      prompt, padding='max_length', max_length=tokenizer.model_max_length,\n",
        "      truncation=True, return_tensors='pt')\n",
        "  with torch.no_grad():\n",
        "    text_embeddings = text_encoder(text_input.input_ids.to(device))[0]\n",
        "\n",
        "  # Do the same for unconditional embeddings\n",
        "  uncond_input = tokenizer(\n",
        "      [''] * len(prompt), padding='max_length',\n",
        "      max_length=tokenizer.model_max_length, return_tensors='pt')\n",
        "  with torch.no_grad():\n",
        "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]\n",
        "\n",
        "  # Concatenate for final embeddings\n",
        "  text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "  return text_embeddings"
      ],
      "metadata": {
        "id": "gZi0UP6Ygr64"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to generate random latents as the latent space needs to be filled with noise so it can later be denoised based on our prompt. In order to denoise the image, some called a scheduler is used and it is essentially designed to scale the denoising process in such a way that it can be completed in the indicated number of inference steps while also maintaining the highest possible quality of the image in those amount of steps. "
      ],
      "metadata": {
        "id": "oU6Xuym23gdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the latent space is all noise, the inference steps begin to take place. How this process works is the Unet model will predict the latents for the next inference step based on the noise and the scheduler. Using this latent data the next step of noise will be predicted. Using this noise, the Unet model will then predict the latents for the next step and the process will repeat itself until the final image is left"
      ],
      "metadata": {
        "id": "CE-eBhfu7sma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def produce_latents(text_embeddings, height=512, width=512,\n",
        "                    num_inference_steps=50, guidance_scale=7.5, latents=None):\n",
        "  if latents is None:\n",
        "    #Generate random latents in order to fill the latent space with noise\n",
        "    latents = torch.randn((text_embeddings.shape[0] // 2, unet.in_channels, \\\n",
        "                           height // 8, width // 8))\n",
        "  latents = latents.to(device)\n",
        "\n",
        "  scheduler.set_timesteps(num_inference_steps)\n",
        "  latents = latents * scheduler.sigmas[0]\n",
        "\n",
        "  with autocast('cuda'):\n",
        "    for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
        "      # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "      latent_model_input = torch.cat([latents] * 2)\n",
        "      sigma = scheduler.sigmas[i]\n",
        "      latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "      # predict the noise residual\n",
        "      with torch.no_grad():\n",
        "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)['sample']\n",
        "\n",
        "      # perform guidance\n",
        "      noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "      noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "      # compute the previous noisy sample x_t -> x_t-1\n",
        "      latents = scheduler.step(noise_pred, i, latents)['prev_sample']\n",
        "  \n",
        "  return latents"
      ],
      "metadata": {
        "id": "6IiNzvrr02iB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model has predicted the latents for the final image, they are put through the decoder and scaled in order to match the desired demensions of the image. "
      ],
      "metadata": {
        "id": "2AOanIFI-tVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_img_latents(latents):\n",
        "  latents = 1 / 0.18215 * latents\n",
        "\n",
        "  with torch.no_grad():\n",
        "    imgs = vae.decode(latents)\n",
        "\n",
        "  imgs = (imgs / 2 + 0.5).clamp(0, 1)\n",
        "  imgs = imgs.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "  imgs = (imgs * 255).round().astype('uint8')\n",
        "  pil_images = [Image.fromarray(image) for image in imgs]\n",
        "  return pil_images\n",
        "\n",
        "# imgs = decode_img_latents(test_latents)\n",
        "# imgs[0]"
      ],
      "metadata": {
        "id": "A6XBqEzl08eN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that all the important functions for the pipeline have been defined, a main pipeline can be made that can call all the important functions in order to run the custom stable diffusion model."
      ],
      "metadata": {
        "id": "N7L7Vzoy_MkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_to_img(prompts, height=512, width=512, num_inference_steps=50,\n",
        "                  guidance_scale=7.5, latents=None):\n",
        "  if isinstance(prompts, str):\n",
        "    prompts = [prompts]\n",
        "\n",
        "  # Prompts -> text embeds\n",
        "  text_embeds = get_text_embeds(prompts)\n",
        "\n",
        "  # Text embeds -> img latents\n",
        "  latents = produce_latents(\n",
        "      text_embeds, height=height, width=width, latents=latents,\n",
        "      num_inference_steps=num_inference_steps, guidance_scale=guidance_scale)\n",
        "  \n",
        "  # Img latents -> imgs\n",
        "  imgs = decode_img_latents(latents)\n",
        "\n",
        "  return imgs"
      ],
      "metadata": {
        "id": "9F8Sjd2_3Fhe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All that is left to do is give a prompt to the model and it will produce the desired image."
      ],
      "metadata": {
        "id": "-ywzYXEF_vE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_to_img('A futuristic city with flying cars', 512, 512, 20)[0]"
      ],
      "metadata": {
        "id": "Xvyhr-lz3INb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the custom pipeline is fully completed!"
      ],
      "metadata": {
        "id": "v5EPHjoCAMMp"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a118bc9bcedc4719995b56f518e90047": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8a27601ceab44c783ead7744a5c1e72",
              "IPY_MODEL_2a35c3978eb840efbfce3e29f118ab22",
              "IPY_MODEL_6078ff4fce5344d1ba732f94ec490e98",
              "IPY_MODEL_30aaf950001848fdbeb41f79a04193ec",
              "IPY_MODEL_3660674291f9416d8c3245660c1b788d"
            ],
            "layout": "IPY_MODEL_f0826dbf791b41eb9364fe931d07069c"
          }
        },
        "e8a27601ceab44c783ead7744a5c1e72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f243a7758e9745e7a57ea625174df41b",
            "placeholder": "​",
            "style": "IPY_MODEL_35cfef600fd047efabcb7792288a4c19",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "2a35c3978eb840efbfce3e29f118ab22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_329a545eda274c6da9d7c18165db137c",
            "placeholder": "​",
            "style": "IPY_MODEL_8bc426b9820a451987a5652bbcb1ff01",
            "value": ""
          }
        },
        "6078ff4fce5344d1ba732f94ec490e98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_545dfb2202f64d3598a88f7fe1251fca",
            "style": "IPY_MODEL_98f274feaea441f4a02a2854e8abfc84",
            "value": true
          }
        },
        "30aaf950001848fdbeb41f79a04193ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_e469c7d4dd4d4bf5b3bf918a800b0d5d",
            "style": "IPY_MODEL_61987676a1fd4a29abd3b0acb4d0a50d",
            "tooltip": ""
          }
        },
        "3660674291f9416d8c3245660c1b788d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a732383f0f3c44688d8651bf895af260",
            "placeholder": "​",
            "style": "IPY_MODEL_42e237105be3436293bf293913aa9836",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "f0826dbf791b41eb9364fe931d07069c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "f243a7758e9745e7a57ea625174df41b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35cfef600fd047efabcb7792288a4c19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "329a545eda274c6da9d7c18165db137c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bc426b9820a451987a5652bbcb1ff01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "545dfb2202f64d3598a88f7fe1251fca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98f274feaea441f4a02a2854e8abfc84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e469c7d4dd4d4bf5b3bf918a800b0d5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61987676a1fd4a29abd3b0acb4d0a50d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "a732383f0f3c44688d8651bf895af260": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42e237105be3436293bf293913aa9836": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}